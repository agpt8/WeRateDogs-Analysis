{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "In the following project I am going to gather and analyze data all around the Twitter account <a href = \"https://twitter.com/dog_rates\">\"WeRateDogs\"</a>. Data is obtained using three different methods - manual download, programmatically download and over an API. After that I am going to assess this data, define the issues found during the assessment and clean these issues to get a cleaned master dataframe. This data will then be analyzed to draw some useful insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sources'></a>\n",
    "## Data Sources\n",
    "\n",
    "\n",
    "1. **Source:** WeRateDogs Twitter Archive (twitter-archive-enhanced.csv)\n",
    "    - Origin: <a href = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv\">Udacity</a>\n",
    "    - Version: Latest (Downloaded 03/05/2020)\n",
    "    - Method of gathering: Manual download\n",
    "\n",
    "\n",
    "2. **Source:** Tweet image predictions (image_predictions.tsv)</li>\n",
    "    - Origin: <a href=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\">Udacity</a>     \n",
    "    - Version: Latest (Downloaded 03/05/2020)\n",
    "    - Method of gathering: Programmatically download via Requests\n",
    "\n",
    "\n",
    "3. **Source:** Additional Twitter data (tweet_json.txt)\n",
    "    - Origin: <a href = \"https://twitter.com/dog_rates\">WeRateDogs</a>   \n",
    "    - Version: Latest (Collected 03/05/2020)\n",
    "    - Method of gathering: API via Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. WeRateDogs Twitter Archive (twitter-archive-enhanced.csv)\n",
    "\n",
    "Since we already have the file, lets verify and view it by importing the contents directly into a dataframe via Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = pd.read_csv(\"./data/raw/twitter-archive-enhanced.csv\")\n",
    "\n",
    "df_twitter.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tweet image predictions (image_predictions.tsv)\n",
    "\n",
    "To gather this data we are going to define the file - url, request this url and write the content of the response to a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "\n",
    "# get response\n",
    "response = requests.get(url)\n",
    "\n",
    "# write return to an image\n",
    "with open(\"./data/raw/image_predictions.tsv\", mode=\"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.read_csv(\"./data/raw/image_predictions.tsv\", sep='\\t')\n",
    "\n",
    "df_predict.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additional Twitter data (tweet_json.txt)\n",
    "\n",
    "To gather the data from the Twitter API I created a Twitter developer account and gathered the data via tweepy. This results in a new file called \"tweet_json.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "consumer_key = '<your key>'\n",
    "consumer_secret = '<your key>'\n",
    "access_token = '<your key>'\n",
    "access_secret = '<your key>'\n",
    "\n",
    "def scrape_twitter_timeline():\n",
    "    # access the API\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    # get all the twitter ids in the df\n",
    "    twitter_ids = list(df_twitter.tweet_id.unique())\n",
    "\n",
    "    # save the gathered data to a file\n",
    "    start = timer()\n",
    "    with open(\"./data/raw/tweet_json.txt\", \"w\") as file:\n",
    "        for ids in twitter_ids:\n",
    "            print(f\"Gather id: {ids}\")\n",
    "            try:\n",
    "                # get all the twitter status - extended mode gives us additional data\n",
    "                tweet = api.get_status(ids, tweet_mode=\"extended\")\n",
    "                # dump the json data to our file\n",
    "                json.dump(tweet._json, file)\n",
    "                # add a linebreak after each dump\n",
    "                file.write('\\n')\n",
    "            except Exception as e:\n",
    "                print(f\"Error - id: {ids}\" + str(e))\n",
    "    end = timer()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in all the necessary data into a dictionary to create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data = []\n",
    "\n",
    "scrape_twitter_timeline()\n",
    "\n",
    "# read the created file\n",
    "with open(\"./data/raw/tweet_json.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            # append a dictionary to the created list\n",
    "            api_data.append({\n",
    "                \"tweet_id\": tweet[\"id\"],\n",
    "                \"retweet_count\": tweet[\"retweet_count\"],\n",
    "                \"favorite_count\": tweet[\"favorite_count\"],\n",
    "                \"retweeted\": tweet[\"retweeted\"],\n",
    "                \"display_text_range\": tweet[\"display_text_range\"]\n",
    "            })\n",
    "\n",
    "            # tweet[\"entities\"][\"media\"][0][\"media_url\"]\n",
    "        except:\n",
    "            print(\"Error.\")\n",
    "\n",
    "df_api = pd.DataFrame(api_data, columns=[\n",
    "                      \"tweet_id\", \"retweet_count\", \"favorite_count\", \"retweeted\", \"display_text_range\"])\n",
    "df_api.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a final check on the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.head(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit5955e1651a714475a4f6b8650f33b461",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}